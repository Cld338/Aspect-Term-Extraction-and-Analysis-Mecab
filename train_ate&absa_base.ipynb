{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/Cld338/Aspect-Term-Extraction-and-Analysis-Custom ATE\n",
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ATE_file = \"bert_ATE_epoch3.pkl\"\n",
    "ABSA_file = \"bert_ABSA_epoch3.pkl\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ATE.model.bert import bert_ATE, bert_ABSA\n",
    "from ATE.data.dataset import dataset_ATM, dataset_ABSA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, ConcatDataset\n",
    "from transformers import BertTokenizer\n",
    "import torch\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import pandas as pd\n",
    "import time\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "pretrain_model_name = \"bert-base-uncased\"\n",
    "tokenizer = BertTokenizer.from_pretrained(pretrain_model_name)\n",
    "lr = 2e-5\n",
    "model_ATE = bert_ATE(pretrain_model_name).to(DEVICE)\n",
    "optimizer_ATE = torch.optim.Adam(model_ATE.parameters(), lr=lr)\n",
    "model_ABSA = bert_ABSA(pretrain_model_name).to(DEVICE)\n",
    "optimizer_ABSA = torch.optim.Adam(model_ABSA.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evl_time(t):\n",
    "    min, sec= divmod(t, 60)\n",
    "    hr, min = divmod(min, 60)\n",
    "    return int(hr), int(min), int(sec)\n",
    "\n",
    "def load_model(model, path):\n",
    "    model.load_state_dict(torch.load(path), strict=False)\n",
    "    return model\n",
    "    \n",
    "def save_model(model, name):\n",
    "    torch.save(model.state_dict(), name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Acpect Term Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "laptops_train_ds = dataset_ATM(pd.read_csv(\"ATE/data/laptops_train.csv\"), tokenizer)\n",
    "laptops_test_ds = dataset_ATM(pd.read_csv(\"ATE/data/laptops_test.csv\"), tokenizer)\n",
    "restaurants_train_ds = dataset_ATM(pd.read_csv(\"ATE/data/restaurants_train.csv\"), tokenizer)\n",
    "restaurants_test_ds = dataset_ATM(pd.read_csv(\"ATE/data/restaurants_test.csv\"), tokenizer)\n",
    "twitter_train_ds = dataset_ATM(pd.read_csv(\"ATE/data/twitter_train.csv\"), tokenizer)\n",
    "twitter_test_ds = dataset_ATM(pd.read_csv(\"ATE/data/twitter_test.csv\"), tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# w,x,y,z = laptops_train_ds.__getitem__(121)\n",
    "# print(w)\n",
    "# print(x)\n",
    "# print(x.size())\n",
    "# print(y)\n",
    "# print(y.size())\n",
    "# print(z)\n",
    "# print(z.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = ConcatDataset([laptops_train_ds, restaurants_train_ds, twitter_train_ds])\n",
    "test_ds = ConcatDataset([laptops_test_ds, restaurants_test_ds, twitter_test_ds])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mini_batch(samples):\n",
    "    ids_tensors = [s[1] for s in samples]\n",
    "    ids_tensors = pad_sequence(ids_tensors, batch_first=True)\n",
    "\n",
    "    tags_tensors = [s[2] for s in samples]\n",
    "    tags_tensors = pad_sequence(tags_tensors, batch_first=True)\n",
    "\n",
    "    pols_tensors = [s[3] for s in samples]\n",
    "    pols_tensors = pad_sequence(pols_tensors, batch_first=True)\n",
    "    \n",
    "    masks_tensors = torch.zeros(ids_tensors.shape, dtype=torch.long)\n",
    "    masks_tensors = masks_tensors.masked_fill(ids_tensors != 0, 1)\n",
    "    \n",
    "    return ids_tensors, tags_tensors, pols_tensors, masks_tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_ds, batch_size=5, collate_fn=create_mini_batch, shuffle = True)\n",
    "test_loader = DataLoader(test_ds, batch_size=50, collate_fn=create_mini_batch, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for batch in train_loader:\n",
    "#     w,x,y,z = batch\n",
    "#     print(w)\n",
    "#     print(w.size())\n",
    "#     print(x)\n",
    "#     print(x.size())\n",
    "#     print(y)\n",
    "#     print(y.size())\n",
    "#     print(z)\n",
    "#     print(z.size())\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_ATE(loader, epochs):\n",
    "    all_data = len(loader)\n",
    "    for epoch in range(epochs):\n",
    "        finish_data = 0\n",
    "        losses = []\n",
    "        current_times = []\n",
    "        correct_predictions = 0\n",
    "        \n",
    "        for data in loader:\n",
    "            t0 = time.time()\n",
    "            ids_tensors, tags_tensors, _, masks_tensors = data\n",
    "            ids_tensors = ids_tensors.to(DEVICE)\n",
    "            tags_tensors = tags_tensors.to(DEVICE)\n",
    "            masks_tensors = masks_tensors.to(DEVICE)\n",
    "\n",
    "            loss = model_ATE(ids_tensors=ids_tensors, tags_tensors=tags_tensors, masks_tensors=masks_tensors)\n",
    "            losses.append(loss.item())\n",
    "            loss.backward()\n",
    "            optimizer_ATE.step()\n",
    "            optimizer_ATE.zero_grad()\n",
    "\n",
    "            finish_data += 1\n",
    "            current_times.append(round(time.time()-t0,3))\n",
    "            current = np.mean(current_times)\n",
    "            hr, min, sec = evl_time(current*(all_data-finish_data) + current*all_data*(epochs-epoch-1))\n",
    "            print('epoch:', epoch, \" batch:\", finish_data, \"/\" , all_data, \" loss:\", np.mean(losses), \" hr:\", hr, \" min:\", min,\" sec:\", sec)         \n",
    "\n",
    "        save_model(model_ATE, f'/content/gdrive/MyDrive/제대_사사과정/{ATE_file}')\n",
    "        \n",
    "def test_model_ATE(loader):\n",
    "    pred = []\n",
    "    trueth = []\n",
    "    with torch.no_grad():\n",
    "        for data in loader:\n",
    "\n",
    "            ids_tensors, tags_tensors, _, masks_tensors = data\n",
    "            ids_tensors = ids_tensors.to(DEVICE)\n",
    "            tags_tensors = tags_tensors.to(DEVICE)\n",
    "            masks_tensors = masks_tensors.to(DEVICE)\n",
    "\n",
    "            outputs = model_ATE(ids_tensors=ids_tensors, tags_tensors=None, masks_tensors=masks_tensors)\n",
    "\n",
    "            _, predictions = torch.max(outputs, dim=2)\n",
    "\n",
    "            pred += list([int(j) for i in predictions for j in i ])\n",
    "            trueth += list([int(j) for i in tags_tensors for j in i ])\n",
    "\n",
    "    return trueth, pred\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "%time train_model_ATE(train_loader, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ATE = load_model(model_ATE, f'/content/gdrive/MyDrive/제대_사사과정/{ATE_file}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 23.1 s\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.99      0.99    140373\n",
      "           1       0.84      0.92      0.88      6486\n",
      "           2       0.93      0.73      0.82      3837\n",
      "\n",
      "    accuracy                           0.98    150696\n",
      "   macro avg       0.92      0.88      0.90    150696\n",
      "weighted avg       0.99      0.98      0.98    150696\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%time x, y = test_model_ATE(test_loader)\n",
    "print(classification_report(x, y, target_names=[str(i) for i in range(3)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aspect Based Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "laptops_train_ds = dataset_ABSA(pd.read_csv(\"ATE/data/laptops_train.csv\"), tokenizer)\n",
    "laptops_test_ds = dataset_ABSA(pd.read_csv(\"ATE/data/laptops_test.csv\"), tokenizer)\n",
    "restaurants_train_ds = dataset_ABSA(pd.read_csv(\"ATE/data/restaurants_train.csv\"), tokenizer)\n",
    "restaurants_test_ds = dataset_ABSA(pd.read_csv(\"ATE/data/restaurants_test.csv\"), tokenizer)\n",
    "twitter_train_ds = dataset_ABSA(pd.read_csv(\"ATE/data/twitter_train.csv\"), tokenizer)\n",
    "twitter_test_ds = dataset_ABSA(pd.read_csv(\"ATE/data/twitter_test.csv\"), tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[cls]', 'the', 'battery', 'life', 'seems', 'to', 'be', 'very', 'good', ',', 'and', 'have', 'had', 'no', 'issues', 'with', 'it', '.', '[sep]', 'battery', 'life']\n",
      "21\n",
      "tensor([ 100, 1996, 6046, 2166, 3849, 2000, 2022, 2200, 2204, 1010, 1998, 2031,\n",
      "        2018, 2053, 3314, 2007, 2009, 1012,  100, 6046, 2166])\n",
      "21\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1])\n",
      "21\n",
      "tensor(2)\n"
     ]
    }
   ],
   "source": [
    "w,x,y,z = laptops_train_ds.__getitem__(121)\n",
    "print(w)\n",
    "print(len(w))\n",
    "print(x)\n",
    "print(len(x))\n",
    "print(y)\n",
    "print(len(y))\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mini_batch2(samples):\n",
    "    ids_tensors = [s[1] for s in samples]\n",
    "    ids_tensors = pad_sequence(ids_tensors, batch_first=True)\n",
    "\n",
    "    segments_tensors = [s[2] for s in samples]\n",
    "    segments_tensors = pad_sequence(segments_tensors, batch_first=True)\n",
    "\n",
    "    label_ids = torch.stack([s[3] for s in samples])\n",
    "    \n",
    "    masks_tensors = torch.zeros(ids_tensors.shape, dtype=torch.long)\n",
    "    masks_tensors = masks_tensors.masked_fill(ids_tensors != 0, 1)\n",
    "\n",
    "    return ids_tensors, segments_tensors, masks_tensors, label_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = ConcatDataset([laptops_train_ds, restaurants_train_ds, twitter_train_ds])\n",
    "test_ds = ConcatDataset([laptops_test_ds, restaurants_test_ds, twitter_test_ds])\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=4, collate_fn=create_mini_batch2, shuffle = True)\n",
    "test_loader = DataLoader(test_ds, batch_size=50, collate_fn=create_mini_batch2, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for batch in train_loader:\n",
    "#     w,x,y,z = batch\n",
    "#     print(w)\n",
    "#     print(w.size())\n",
    "#     print(x)\n",
    "#     print(x.size())\n",
    "#     print(y)\n",
    "#     print(y.size())\n",
    "#     print(z)\n",
    "#     print(z.size())\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_ABSA(loader, epochs):\n",
    "    all_data = len(loader)\n",
    "    for epoch in range(epochs):\n",
    "        finish_data = 0\n",
    "        losses = []\n",
    "        current_times = []\n",
    "        correct_predictions = 0\n",
    "        \n",
    "        for data in loader:\n",
    "            t0 = time.time()\n",
    "            ids_tensors, segments_tensors, masks_tensors, label_ids = data\n",
    "            ids_tensors = ids_tensors.to(DEVICE)\n",
    "            segments_tensors = segments_tensors.to(DEVICE)\n",
    "            label_ids = label_ids.to(DEVICE)\n",
    "            masks_tensors = masks_tensors.to(DEVICE)\n",
    "\n",
    "            loss = model_ABSA(ids_tensors=ids_tensors, lable_tensors=label_ids, masks_tensors=masks_tensors, segments_tensors=segments_tensors)\n",
    "            losses.append(loss.item())\n",
    "            loss.backward()\n",
    "            optimizer_ABSA.step()\n",
    "            optimizer_ABSA.zero_grad()\n",
    "\n",
    "            finish_data += 1\n",
    "            current_times.append(round(time.time()-t0,3))\n",
    "            current = np.mean(current_times)\n",
    "            hr, min, sec = evl_time(current*(all_data-finish_data) + current*all_data*(epochs-epoch-1))\n",
    "            print('epoch:', epoch, \" batch:\", finish_data, \"/\" , all_data, \" loss:\", np.mean(losses), \" hr:\", hr, \" min:\", min,\" sec:\", sec)         \n",
    "\n",
    "        save_model(model_ABSA, f'/content/gdrive/MyDrive/제대_사사과정/{ABSA_file}')\n",
    "        \n",
    "def test_model_ABSA(loader):\n",
    "    pred = []\n",
    "    trueth = []\n",
    "    with torch.no_grad():\n",
    "        for data in loader:\n",
    "\n",
    "            ids_tensors, segments_tensors, masks_tensors, label_ids = data\n",
    "            ids_tensors = ids_tensors.to(DEVICE)\n",
    "            segments_tensors = segments_tensors.to(DEVICE)\n",
    "            masks_tensors = masks_tensors.to(DEVICE)\n",
    "\n",
    "            outputs = model_ABSA(ids_tensors, None, masks_tensors=masks_tensors, segments_tensors=segments_tensors)\n",
    "            \n",
    "            _, predictions = torch.max(outputs, dim=1)\n",
    "\n",
    "            pred += list([int(i) for i in predictions])\n",
    "            trueth += list([int(i) for i in label_ids])\n",
    "\n",
    "    return trueth, pred\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time train_model_ABSA(train_loader, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ABSA = load_model(model_ABSA, f'/content/gdrive/MyDrive/제대_사사과정/{ABSA_file}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 10.1 s\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.75      0.74       497\n",
      "           1       0.67      0.74      0.70       710\n",
      "           2       0.89      0.83      0.86      1239\n",
      "\n",
      "    accuracy                           0.79      2446\n",
      "   macro avg       0.76      0.77      0.77      2446\n",
      "weighted avg       0.79      0.79      0.79      2446\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%time x, y = test_model_ABSA(test_loader)\n",
    "print(classification_report(x, y, target_names=[str(i) for i in range(3)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ATE + ABSA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_model_ABSA(sentence, aspect, tokenizer):\n",
    "    t1 = tokenizer.tokenize(sentence)\n",
    "    t2 = tokenizer.tokenize(aspect)\n",
    "\n",
    "    word_pieces = ['[cls]']\n",
    "    word_pieces += t1\n",
    "    word_pieces += ['[sep]']\n",
    "    word_pieces += t2\n",
    "\n",
    "    segment_tensor = [0] + [0]*len(t1) + [0] + [1]*len(t2)\n",
    "\n",
    "    ids = tokenizer.convert_tokens_to_ids(word_pieces)\n",
    "    input_tensor = torch.tensor([ids]).to(DEVICE)\n",
    "    segment_tensor = torch.tensor(segment_tensor).to(DEVICE)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model_ABSA(input_tensor, None, None, segments_tensors=segment_tensor)\n",
    "        _, predictions = torch.max(outputs, dim=1)\n",
    "    \n",
    "    return word_pieces, predictions, outputs\n",
    "\n",
    "def predict_model_ATE(sentence, tokenizer):\n",
    "    word_pieces = []\n",
    "    tokens = tokenizer.tokenize(sentence)\n",
    "    word_pieces += tokens\n",
    "\n",
    "    ids = tokenizer.convert_tokens_to_ids(word_pieces)\n",
    "    input_tensor = torch.tensor([ids]).to(DEVICE)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model_ATE(input_tensor, None, None)\n",
    "        _, predictions = torch.max(outputs, dim=2)\n",
    "    predictions = predictions[0].tolist()\n",
    "\n",
    "    return word_pieces, predictions, outputs\n",
    "\n",
    "def ATE_ABSA(text):\n",
    "    terms = []\n",
    "    word = \"\"\n",
    "    x, y, z = predict_model_ATE(text, tokenizer)\n",
    "    for i in range(len(y)):\n",
    "        if y[i] == 1:\n",
    "            if len(word) != 0:\n",
    "                terms.append(word.replace(\" ##\",\"\"))\n",
    "            word = x[i]\n",
    "        if y[i] == 2:\n",
    "            word += (\" \" + x[i])\n",
    "            \n",
    "    \n",
    "    if len(word) != 0:\n",
    "            terms.append(word.replace(\" ##\",\"\"))\n",
    "            \n",
    "    print(\"tokens:\", x)\n",
    "    print(\"ATE:\", terms)\n",
    "    \n",
    "    if len(terms) != 0:\n",
    "        for i in terms:\n",
    "            _, c, p = predict_model_ABSA(text, i, tokenizer)\n",
    "            print(\"term:\", [i], \"class:\", [int(c)], \"ABSA:\", [float(p[0][0]), float(p[0][1]), float(p[0][2])])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ABSA = load_model(model_ABSA, f'/content/gdrive/MyDrive/제대_사사과정/{ABSA_file}')\n",
    "model_ATE = load_model(model_ATE, f'/content/gdrive/MyDrive/제대_사사과정/{ATE_file}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens: ['for', 'the', 'price', 'you', 'pay', 'this', 'product', 'is', 'very', 'good', '.', 'however', ',', 'battery', 'life', 'is', 'a', 'little', 'lack', '-', 'lust', '##er', 'coming', 'from', 'a', 'mac', '##book', 'pro', '.']\n",
      "ATE: ['price', 'battery life']\n",
      "term: ['price'] class: [2] ABSA: [-2.057527542114258, -0.6292028427124023, 2.606888771057129]\n",
      "term: ['battery life'] class: [0] ABSA: [5.0118207931518555, -2.3663508892059326, -1.8548927307128906]\n"
     ]
    }
   ],
   "source": [
    "text = \"For the price you pay this product is very good. However, battery life is a little lack-luster coming from a MacBook Pro.\"\n",
    "ATE_ABSA(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens: ['i', 'think', 'apple', 'is', 'better', 'than', 'microsoft', '.']\n",
      "ATE: ['apple', 'microsoft']\n",
      "term: ['apple'] class: [1] ABSA: [-0.6019173264503479, 1.7071634531021118, -0.6558032631874084]\n",
      "term: ['microsoft'] class: [0] ABSA: [3.7963366508483887, 0.2745559811592102, -3.170161485671997]\n"
     ]
    }
   ],
   "source": [
    "text = \"I think Apple is better than Microsoft.\"\n",
    "ATE_ABSA(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cyberpunk 2077 - Xbox One\n",
    "\n",
    "https://www.amazon.com/-/zh_TW/Cyberpunk-2077-Xbox-One/product-reviews/B07DJW4WZC/ref=cm_cr_arp_d_paging_btm_next_2?ie=UTF8&reviewerType=all_reviews&pageNumber=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens: ['spent', '5', 'hours', 'download', '##ing', 'updates', '.']\n",
      "ATE: ['download', '##ing updates']\n",
      "term: ['download'] class: [1] ABSA: [-1.8432576656341553, 5.523550987243652, -3.1730458736419678]\n",
      "term: ['##ing updates'] class: [1] ABSA: [-2.6808316707611084, 6.011416912078857, -2.864603042602539]\n"
     ]
    }
   ],
   "source": [
    "text = \"Spent 5 hours downloading updates.\"\n",
    "ATE_ABSA(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens: ['install', 'is', 'bug', '##gy', ',', 'so', 'after', 'download', '##ing', 'a', 'day', 'one', 'patch', 'that', \"'\", 's', 'nearly', '3', 'times', 'the', 'size', 'of', 'the', 'game', ',', 'it', 'g', '##lit', '##ched', 'on', 'the', 'cds', 'and', 'had', 'to', 'reins', '##tal', '##l', 'the', 'game', 'from', 'scratch', '.']\n",
      "ATE: ['install', 'patch', 'size', 'game', 'cds', 'game']\n",
      "term: ['install'] class: [1] ABSA: [-2.2875146865844727, 4.987157821655273, -2.462806463241577]\n",
      "term: ['patch'] class: [0] ABSA: [3.8425865173339844, -1.115385890007019, -1.8236310482025146]\n",
      "term: ['size'] class: [1] ABSA: [1.2320547103881836, 1.6156060695648193, -2.0953636169433594]\n",
      "term: ['game'] class: [1] ABSA: [-2.642533779144287, 5.77440071105957, -2.5892865657806396]\n",
      "term: ['cds'] class: [1] ABSA: [-2.673746347427368, 5.815953731536865, -2.6103947162628174]\n",
      "term: ['game'] class: [1] ABSA: [-2.642533779144287, 5.77440071105957, -2.5892865657806396]\n"
     ]
    }
   ],
   "source": [
    "text = \"Install is buggy, so after downloading a day one patch that's nearly 3 times the size of the game, it glitched on the CDs and had to reinstall the game from scratch.\"\n",
    "ATE_ABSA(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens: ['cyber', '##pu', '##nk', '207', '##7', 'freeze', '##s', 'constantly', ',', 'frame', 'rates', 'are', 'terrible', ',', 'and', 'it', \"'\", 's', 'extremely', 'frustrating', 'to', 'try', 'to', 'play', '.']\n",
      "ATE: ['cyberpu', 'frame rates']\n",
      "term: ['cyberpu'] class: [0] ABSA: [4.44415283203125, -0.36560752987861633, -3.3459084033966064]\n",
      "term: ['frame rates'] class: [0] ABSA: [5.2562408447265625, -2.305537700653076, -2.0652124881744385]\n"
     ]
    }
   ],
   "source": [
    "text = \"Cyberpunk 2077 freezes constantly, frame rates are terrible, and it's extremely frustrating to try to play.\"\n",
    "ATE_ABSA(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens: ['cyber', '##pu', '##nk', '207', '##7', 'is', 'completely', 'un', '##play', '##able', 'on', 'xbox', 'one', '.', 'they', 'should', 'have', 'never', 'released', 'this', 'for', 'current', 'gen', '.']\n",
      "ATE: ['xbox']\n",
      "term: ['xbox'] class: [1] ABSA: [-2.0123980045318604, 5.7579731941223145, -3.235884666442871]\n"
     ]
    }
   ],
   "source": [
    "text = \"Cyberpunk 2077 is completely unplayable on xbox one. They should have never released this for current gen.\"\n",
    "ATE_ABSA(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens: ['it', '’', 's', 'just', 'a', 'cash', 'grab', ',', 'the', 'game', 'crashes', 'constantly', ',', 'runs', 'at', 'like', '20', 'f', '##ps', ',', 'half', 'the', 'environment', 'and', 'characters', 'only', 'load', 'when', 'you', '’', 're', 'three', 'feet', 'away', 'from', 'them', '.', 'unless', 'you', '’', 're', 'in', 'a', 'small', 'space', 'the', 'game', 'looks', 'awful', '.', 'the', 'worst', 'game', 'i', '’', 've', 'ever', 'played', 'in', 'years', 'visually', '.', 'it', 'looks', 'worse', 'than', 'later', 'xbox', '360', 'games', '.']\n",
      "ATE: ['runs', 'environment', 'characters', 'xbox 360']\n",
      "term: ['runs'] class: [0] ABSA: [4.292036056518555, -0.47526031732559204, -3.092445135116577]\n",
      "term: ['environment'] class: [0] ABSA: [3.7919883728027344, 0.2691774070262909, -3.255924940109253]\n",
      "term: ['characters'] class: [0] ABSA: [3.6958675384521484, 0.34112149477005005, -3.180785655975342]\n",
      "term: ['xbox 360'] class: [0] ABSA: [3.6143949031829834, 0.5955154299736023, -3.378544330596924]\n"
     ]
    }
   ],
   "source": [
    "text = \"It’s just a cash grab, the game crashes constantly, runs at like 20 fps, half the environment and characters only load when you’re three feet away from them. Unless you’re in a small space the game looks awful. The worst game i’ve ever played in years visually. It looks worse than later xbox 360 games.\"\n",
    "ATE_ABSA(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens: ['cd', 'pro', '##je', '##kt', 'red', 'should', 'have', 'just', 'abandoned', 'the', 'current', 'gen', 'consoles', 'instead', 'of', 'cheating', 'people', 'out', 'of', 'their', 'money', '.']\n",
      "ATE: ['gen', 'consoles']\n",
      "term: ['gen'] class: [1] ABSA: [-2.468400478363037, 5.807504177093506, -2.8556580543518066]\n",
      "term: ['consoles'] class: [1] ABSA: [-2.6495633125305176, 5.937881946563721, -2.8111135959625244]\n"
     ]
    }
   ],
   "source": [
    "text = \"CD Projekt Red should have just abandoned the current gen consoles instead of cheating people out of their money.\"\n",
    "ATE_ABSA(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
