{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0AW1cyh43s9G",
        "outputId": "04c4c9a1-8fa8-448a-e179-287c72a2d7c2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "fatal: destination path 'ATE' already exists and is not an empty directory.\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.35.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.0)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n",
            "Collecting git+https://****@github.com/SKTBrain/KoBERT.git@master\n",
            "  Cloning https://****@github.com/SKTBrain/KoBERT.git (to revision master) to /tmp/pip-req-build-9ukyueeb\n",
            "  Running command git clone --filter=blob:none --quiet 'https://****@github.com/SKTBrain/KoBERT.git' /tmp/pip-req-build-9ukyueeb\n",
            "  Resolved https://****@github.com/SKTBrain/KoBERT.git to commit 47a69af87928fc24e20f571fe10c3cc9dd9af9a3\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting boto3<=1.15.18 (from kobert==0.2.3)\n",
            "  Using cached boto3-1.15.18-py2.py3-none-any.whl (129 kB)\n",
            "Collecting gluonnlp<=0.10.0,>=0.6.0 (from kobert==0.2.3)\n",
            "  Using cached gluonnlp-0.10.0.tar.gz (344 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting mxnet<=1.7.0.post2,>=1.4.0 (from kobert==0.2.3)\n",
            "  Using cached mxnet-1.7.0.post2-py2.py3-none-manylinux2014_x86_64.whl (54.7 MB)\n",
            "INFO: pip is looking at multiple versions of kobert to determine which version is compatible with other requirements. This could take a while.\n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement onnxruntime<=1.8.0,==1.8.0 (from kobert) (from versions: 1.12.0, 1.12.1, 1.13.1, 1.14.0, 1.14.1, 1.15.0, 1.15.1, 1.16.0, 1.16.1, 1.16.2)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for onnxruntime<=1.8.0,==1.8.0\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/Cld338/Aspect-Term-Extraction-and-Analysis-Mecab ATE\n",
        "!pip install transformers\n",
        "!python -m pip install git+https://git@github.com/SKTBrain/KoBERT.git@master"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QrO8w-rvKUJe",
        "outputId": "8c8fca27-5b24-4c4f-b6ad-6c95fd306385"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: jsonlines in /usr/local/lib/python3.10/dist-packages (4.0.0)\n",
            "Requirement already satisfied: attrs>=19.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonlines) (23.1.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install jsonlines"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FJzMr2HVKWtH",
        "outputId": "07e2ae88-27c8-47cc-dc98-2a116272872b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\r0% [Working]\r            \rHit:1 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease\n",
            "Hit:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "Get:3 http://security.ubuntu.com/ubuntu jammy-security InRelease [110 kB]\n",
            "Hit:4 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Get:5 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [119 kB]\n",
            "Hit:6 https://ppa.launchpadcontent.net/c2d4u.team/c2d4u4.0+/ubuntu jammy InRelease\n",
            "Hit:7 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:8 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:9 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Hit:10 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
            "Fetched 229 kB in 2s (116 kB/s)\n",
            "Reading package lists... Done\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "g++ is already the newest version (4:11.2.0-1ubuntu1).\n",
            "openjdk-8-jdk is already the newest version (8u382-ga-1~22.04.1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 14 not upgraded.\n",
            "Requirement already satisfied: konlpy in /usr/local/lib/python3.10/dist-packages (0.6.0)\n",
            "Requirement already satisfied: JPype1-py3 in /usr/local/lib/python3.10/dist-packages (0.5.5.4)\n",
            "Requirement already satisfied: JPype1>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from konlpy) (1.4.1)\n",
            "Requirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.10/dist-packages (from konlpy) (4.9.3)\n",
            "Requirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.10/dist-packages (from konlpy) (1.23.5)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from JPype1>=0.7.0->konlpy) (23.2)\n",
            "mecab-ko is already installed\n",
            "mecab-ko-dic is already installed\n",
            "mecab-python is already installed\n",
            "Done.\n"
          ]
        }
      ],
      "source": [
        "!apt-get update\n",
        "!apt-get install g++ openjdk-8-jdk\n",
        "!pip3 install konlpy JPype1-py3\n",
        "!bash <(curl -s https://raw.githubusercontent.com/konlpy/konlpy/master/scripts/mecab.sh)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h1PNM5BeKcE8",
        "outputId": "283c6b09-3f90-4bd1-a70a-49cdf294b514"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: mecab-python3 in /usr/local/lib/python3.10/dist-packages (1.0.8)\n"
          ]
        }
      ],
      "source": [
        "!pip install mecab-python3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1TPhpe2h3s9N"
      },
      "source": [
        "# import"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1PJ_AWhP3s9Q",
        "outputId": "b616f205-fe4f-4f54-d6af-92e1f7bbe1aa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive/; to attempt to forcibly remount, call drive.mount(\"/content/gdrive/\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive/')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "G415cI7a3s9R"
      },
      "outputs": [],
      "source": [
        "from ATE.model.bert import bert_ATE, bert_ABSA\n",
        "from ATE.data.dataset import dataset_ATM, dataset_ABSA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "HQfQngWS3s9R"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader, ConcatDataset\n",
        "from transformers import BertTokenizer\n",
        "import torch\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "import pandas as pd\n",
        "import time\n",
        "import numpy as np\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import confusion_matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "u7S9zvff3s9M"
      },
      "outputs": [],
      "source": [
        "epoch = 15\n",
        "ATE_file = f\"bert_ATE_epoch{epoch}.pkl\"\n",
        "ABSA_file = f\"bert_ABSA_epoch{epoch}.pkl\"\n",
        "file_save_path = \"/content/gdrive/MyDrive/2023_sw_inje_proj/bert-base-multilingual-uncased\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oDppg5so3s9S"
      },
      "source": [
        "# Settings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "YVCgrlV23s9S"
      },
      "outputs": [],
      "source": [
        "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "pretrain_model_name = \"bert-base-multilingual-uncased\"\n",
        "tokenizer = BertTokenizer.from_pretrained(pretrain_model_name)\n",
        "lr = 2e-5\n",
        "model_ATE = bert_ATE(pretrain_model_name).to(DEVICE)\n",
        "optimizer_ATE = torch.optim.Adam(model_ATE.parameters(), lr=lr)\n",
        "model_ABSA = bert_ABSA(pretrain_model_name).to(DEVICE)\n",
        "optimizer_ABSA = torch.optim.Adam(model_ABSA.parameters(), lr=lr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "KRh2Vo4TKhBu"
      },
      "outputs": [],
      "source": [
        "from konlpy.tag import Mecab\n",
        "mecab = Mecab()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "r3n-AP5k3s9T"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "def evl_time(t):\n",
        "    min, sec= divmod(t, 60)\n",
        "    hr, min = divmod(min, 60)\n",
        "    return int(hr), int(min), int(sec)\n",
        "\n",
        "def load_model(model, path):\n",
        "    model.load_state_dict(torch.load(path), strict=False)\n",
        "    return model\n",
        "\n",
        "def save_model(model, name):\n",
        "    torch.save(model.state_dict(), name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i4MUsvAI3s9T"
      },
      "source": [
        "# Acpect Term Extraction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "5nxUAfV63s9U"
      },
      "outputs": [],
      "source": [
        "# laptops_train_ds = dataset_ATM(pd.read_csv(\"ATE/data/laptops_train.csv\"), tokenizer)\n",
        "# laptops_test_ds = dataset_ATM(pd.read_csv(\"ATE/data/laptops_test.csv\"), tokenizer)\n",
        "# restaurants_train_ds = dataset_ATM(pd.read_csv(\"ATE/data/restaurants_train.csv\"), tokenizer)\n",
        "# restaurants_test_ds = dataset_ATM(pd.read_csv(\"ATE/data/restaurants_test.csv\"), tokenizer)\n",
        "# twitter_train_ds = dataset_ATM(pd.read_csv(\"ATE/data/twitter_train.csv\"), tokenizer)\n",
        "# twitter_test_ds = dataset_ATM(pd.read_csv(\"ATE/data/twitter_test.csv\"), tokenizer)\n",
        "data_path = \"/content/gdrive/MyDrive/2023_sw_inje_proj/data\"\n",
        "custom_train_ds = dataset_ATM(pd.read_csv(f\"{data_path}/processed_nikluge-sa-2022-train.csv\"), mecab)\n",
        "custom_test_ds = dataset_ATM(pd.read_csv(f\"{data_path}/processed_nikluge-sa-2022-dev.csv\"), mecab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "_nSm9KiW3s9U"
      },
      "outputs": [],
      "source": [
        "# w,x,y,z = laptops_train_ds.__getitem__(121)\n",
        "# print(w)\n",
        "# print(x)\n",
        "# print(x.size())\n",
        "# print(y)\n",
        "# print(y.size())\n",
        "# print(z)\n",
        "# print(z.size())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "vrdo33723s9U"
      },
      "outputs": [],
      "source": [
        "\n",
        "# train_ds = ConcatDataset([laptops_train_ds, restaurants_train_ds, twitter_train_ds])\n",
        "# test_ds = ConcatDataset([laptops_test_ds, restaurants_test_ds, twitter_test_ds])\n",
        "train_ds = ConcatDataset([custom_train_ds])\n",
        "test_ds = ConcatDataset([custom_train_ds])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "gYM0_Exp3s9U"
      },
      "outputs": [],
      "source": [
        "def create_mini_batch(samples):\n",
        "    ids_tensors = [s[1] for s in samples]\n",
        "    ids_tensors = pad_sequence(ids_tensors, batch_first=True)\n",
        "\n",
        "    tags_tensors = [s[2] for s in samples]\n",
        "    tags_tensors = pad_sequence(tags_tensors, batch_first=True)\n",
        "\n",
        "    pols_tensors = [s[3] for s in samples]\n",
        "    pols_tensors = pad_sequence(pols_tensors, batch_first=True)\n",
        "\n",
        "    masks_tensors = torch.zeros(ids_tensors.shape, dtype=torch.long)\n",
        "    masks_tensors = masks_tensors.masked_fill(ids_tensors != 0, 1)\n",
        "\n",
        "    return ids_tensors, tags_tensors, pols_tensors, masks_tensors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "tBrdXt7a3s9V"
      },
      "outputs": [],
      "source": [
        "train_loader = DataLoader(train_ds, batch_size=5, collate_fn=create_mini_batch, shuffle = True)\n",
        "test_loader = DataLoader(test_ds, batch_size=50, collate_fn=create_mini_batch, shuffle = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "ijrReQcx3s9V"
      },
      "outputs": [],
      "source": [
        "# for batch in train_loader:\n",
        "#     w,x,y,z = batch\n",
        "#     print(w)\n",
        "#     print(w.size())\n",
        "#     print(x)\n",
        "#     print(x.size())\n",
        "#     print(y)\n",
        "#     print(y.size())\n",
        "#     print(z)\n",
        "#     print(z.size())\n",
        "#     break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "NxByyQ053s9V"
      },
      "outputs": [],
      "source": [
        "def train_model_ATE(loader, epochs):\n",
        "    all_data = len(loader)\n",
        "    for epoch in range(epochs):\n",
        "        finish_data = 0\n",
        "        losses = []\n",
        "        current_times = []\n",
        "        correct_predictions = 0\n",
        "\n",
        "        for data in loader:\n",
        "            t0 = time.time()\n",
        "            ids_tensors, tags_tensors, _,  masks_tensors = data\n",
        "            ids_tensors = ids_tensors.to(DEVICE)\n",
        "            tags_tensors = tags_tensors.to(DEVICE)\n",
        "            masks_tensors = masks_tensors.to(DEVICE)\n",
        "\n",
        "            loss = model_ATE(ids_tensors=ids_tensors, tags_tensors=tags_tensors, masks_tensors=masks_tensors)\n",
        "            losses.append(loss.item())\n",
        "            loss.backward()\n",
        "            optimizer_ATE.step()\n",
        "            optimizer_ATE.zero_grad()\n",
        "\n",
        "            finish_data += 1\n",
        "            current_times.append(round(time.time()-t0,3))\n",
        "            current = np.mean(current_times)\n",
        "            hr, min, sec = evl_time(current*(all_data-finish_data) + current*all_data*(epochs-epoch-1))\n",
        "            print('epoch:', epoch, \" batch:\", finish_data, \"/\" , all_data, \" loss:\", np.mean(losses), \" hr:\", hr, \" min:\", min,\" sec:\", sec)\n",
        "\n",
        "        save_model(model_ATE, f'{file_save_path}/{ATE_file}')\n",
        "\n",
        "def test_model_ATE(loader):\n",
        "    pred = []\n",
        "    trueth = []\n",
        "    with torch.no_grad():\n",
        "        for data in loader:\n",
        "\n",
        "            ids_tensors, tags_tensors, _, masks_tensors = data\n",
        "            ids_tensors = ids_tensors.to(DEVICE)\n",
        "            tags_tensors = tags_tensors.to(DEVICE)\n",
        "            masks_tensors = masks_tensors.to(DEVICE)\n",
        "\n",
        "            outputs = model_ATE(ids_tensors=ids_tensors, tags_tensors=None, masks_tensors=masks_tensors)\n",
        "\n",
        "            _, predictions = torch.max(outputs, dim=2)\n",
        "\n",
        "            pred += list([int(j) for i in predictions for j in i ])\n",
        "            trueth += list([int(j) for i in tags_tensors for j in i ])\n",
        "\n",
        "    return trueth, pred\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "4kLFks-E3s9V"
      },
      "outputs": [],
      "source": [
        "# %time train_model_ATE(train_loader, epoch)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "sZfx6LSu3s9V"
      },
      "outputs": [],
      "source": [
        "model_ATE = load_model(model_ATE, f'{file_save_path}/{ATE_file}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "L5MUmN0A3s9W",
        "outputId": "c85cb3b2-7e83-4625-fde6-1abeed8ba2c3"
      },
      "outputs": [
        {
          "ename": "AttributeError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-57-3c1a55304b54>\u001b[0m in \u001b[0;36mtest_model_ATE\u001b[0;34m(loader)\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0mtrueth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mloader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m             \u001b[0mids_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtags_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmasks_tensors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    628\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    629\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 630\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    631\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    632\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    672\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    673\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 674\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    675\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    676\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataset.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    300\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    301\u001b[0m             \u001b[0msample_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0midx\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcumulative_sizes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdataset_idx\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 302\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatasets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdataset_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msample_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    304\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/ATE/data/dataset.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mbert_pols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m             \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m             \u001b[0mbert_tokens\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m             \u001b[0mbert_tags\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtags\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'Mecab' object has no attribute 'tokenize'"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.64      0.74      0.69       497\n",
            "           1       0.73      0.59      0.65       710\n",
            "           2       0.83      0.87      0.85      1239\n",
            "\n",
            "    accuracy                           0.76      2446\n",
            "   macro avg       0.73      0.73      0.73      2446\n",
            "weighted avg       0.76      0.76      0.76      2446\n",
            "\n"
          ]
        }
      ],
      "source": [
        "%time x, y = test_model_ATE(test_loader)\n",
        "print(classification_report(x, y, target_names=[str(i) for i in range(3)]))\n",
        "with open(f\"{file_save_path}/ATE_epoch{epoch}_report.txt\", \"w\") as file:\n",
        "  file.write(classification_report(x, y, target_names=[str(i) for i in range(3)]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k3MZ6MeV3s9W"
      },
      "source": [
        "# Aspect Based Sentiment Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "AqbE24g33s9W"
      },
      "outputs": [],
      "source": [
        "laptops_train_ds = dataset_ABSA(pd.read_csv(\"ATE/data/laptops_train.csv\"), tokenizer)\n",
        "laptops_test_ds = dataset_ABSA(pd.read_csv(\"ATE/data/laptops_test.csv\"), tokenizer)\n",
        "restaurants_train_ds = dataset_ABSA(pd.read_csv(\"ATE/data/restaurants_train.csv\"), tokenizer)\n",
        "restaurants_test_ds = dataset_ABSA(pd.read_csv(\"ATE/data/restaurants_test.csv\"), tokenizer)\n",
        "twitter_train_ds = dataset_ABSA(pd.read_csv(\"ATE/data/twitter_train.csv\"), tokenizer)\n",
        "twitter_test_ds = dataset_ABSA(pd.read_csv(\"ATE/data/twitter_test.csv\"), tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p2-oGjCB3s9X",
        "outputId": "12936b24-08ea-40a3-af67-c74f12457f43"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['[cls]', 'the', 'battery', 'life', 'seems', 'to', 'be', 'very', 'good', ',', 'and', 'have', 'had', 'no', 'issues', 'with', 'it', '.', '[sep]', 'battery', 'life']\n",
            "21\n",
            "tensor([  100, 10103, 34794, 10287, 32681, 10114, 10346, 12495, 12050,   117,\n",
            "        10110, 10574, 10407, 10181, 17156, 10171, 10197,   119,   100, 34794,\n",
            "        10287])\n",
            "21\n",
            "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1])\n",
            "21\n",
            "tensor(2)\n"
          ]
        }
      ],
      "source": [
        "w,x,y,z = laptops_train_ds.__getitem__(121)\n",
        "print(w)\n",
        "print(len(w))\n",
        "print(x)\n",
        "print(len(x))\n",
        "print(y)\n",
        "print(len(y))\n",
        "print(z)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "EJtanda33s9X"
      },
      "outputs": [],
      "source": [
        "def create_mini_batch2(samples):\n",
        "    ids_tensors = [s[1] for s in samples]\n",
        "    ids_tensors = pad_sequence(ids_tensors, batch_first=True)\n",
        "\n",
        "    segments_tensors = [s[2] for s in samples]\n",
        "    segments_tensors = pad_sequence(segments_tensors, batch_first=True)\n",
        "\n",
        "    label_ids = torch.stack([s[3] for s in samples])\n",
        "\n",
        "    masks_tensors = torch.zeros(ids_tensors.shape, dtype=torch.long)\n",
        "    masks_tensors = masks_tensors.masked_fill(ids_tensors != 0, 1)\n",
        "\n",
        "    return ids_tensors, segments_tensors, masks_tensors, label_ids"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "sRHXh9jb3s9X"
      },
      "outputs": [],
      "source": [
        "train_ds = ConcatDataset([laptops_train_ds, restaurants_train_ds, twitter_train_ds])\n",
        "test_ds = ConcatDataset([laptops_test_ds, restaurants_test_ds, twitter_test_ds])\n",
        "\n",
        "train_loader = DataLoader(train_ds, batch_size=4, collate_fn=create_mini_batch2, shuffle = True)\n",
        "test_loader = DataLoader(test_ds, batch_size=50, collate_fn=create_mini_batch2, shuffle = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "id": "89mMsDNb3s9X"
      },
      "outputs": [],
      "source": [
        "# for batch in train_loader:\n",
        "#     w,x,y,z = batch\n",
        "#     print(w)\n",
        "#     print(w.size())\n",
        "#     print(x)\n",
        "#     print(x.size())\n",
        "#     print(y)\n",
        "#     print(y.size())\n",
        "#     print(z)\n",
        "#     print(z.size())\n",
        "#     break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "id": "zftJjfMQ3s9X"
      },
      "outputs": [],
      "source": [
        "def train_model_ABSA(loader, epochs):\n",
        "    all_data = len(loader)\n",
        "    for epoch in range(epochs):\n",
        "        finish_data = 0\n",
        "        losses = []\n",
        "        current_times = []\n",
        "        correct_predictions = 0\n",
        "\n",
        "        for data in loader:\n",
        "            t0 = time.time()\n",
        "            ids_tensors, segments_tensors, masks_tensors, label_ids = data\n",
        "            ids_tensors = ids_tensors.to(DEVICE)\n",
        "            segments_tensors = segments_tensors.to(DEVICE)\n",
        "            label_ids = label_ids.to(DEVICE)\n",
        "            masks_tensors = masks_tensors.to(DEVICE)\n",
        "\n",
        "            loss = model_ABSA(ids_tensors=ids_tensors, lable_tensors=label_ids, masks_tensors=masks_tensors, segments_tensors=segments_tensors)\n",
        "            losses.append(loss.item())\n",
        "            loss.backward()\n",
        "            optimizer_ABSA.step()\n",
        "            optimizer_ABSA.zero_grad()\n",
        "\n",
        "            finish_data += 1\n",
        "            current_times.append(round(time.time()-t0,3))\n",
        "            current = np.mean(current_times)\n",
        "            hr, min, sec = evl_time(current*(all_data-finish_data) + current*all_data*(epochs-epoch-1))\n",
        "            print('epoch:', epoch, \" batch:\", finish_data, \"/\" , all_data, \" loss:\", np.mean(losses), \" hr:\", hr, \" min:\", min,\" sec:\", sec)\n",
        "\n",
        "        save_model(model_ABSA, f'{file_save_path}/{ABSA_file}')\n",
        "\n",
        "def test_model_ABSA(loader):\n",
        "    pred = []\n",
        "    trueth = []\n",
        "    with torch.no_grad():\n",
        "        for data in loader:\n",
        "\n",
        "            ids_tensors, segments_tensors, masks_tensors, label_ids = data\n",
        "            ids_tensors = ids_tensors.to(DEVICE)\n",
        "            segments_tensors = segments_tensors.to(DEVICE)\n",
        "            masks_tensors = masks_tensors.to(DEVICE)\n",
        "\n",
        "            outputs = model_ABSA(ids_tensors, None, masks_tensors=masks_tensors, segments_tensors=segments_tensors)\n",
        "\n",
        "            _, predictions = torch.max(outputs, dim=1)\n",
        "\n",
        "            pred += list([int(i) for i in predictions])\n",
        "            trueth += list([int(i) for i in label_ids])\n",
        "\n",
        "    return trueth, pred\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I2sdCp4eBKvk",
        "outputId": "2044a491-cc9e-4855-8c52-f39cb29fbb0c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(tensor([[  100,   143, 10981, 15640, 52245, 10103, 34794, 10359, 10127, 10816,\n",
            "         28032, 23578, 10146, 13594,   143, 31789, 10287, 10108, 10170, 18030,\n",
            "         10502, 10704, 26344, 36555, 15503, 10902,   128, 18030, 10108, 11416,\n",
            "           119,   100, 34794,     0,     0,     0],\n",
            "        [  100, 10387, 10159,   119,   119, 10578, 10346, 11746, 14666,   119,\n",
            "           119, 39643, 10935,   107,   107, 11811, 46646, 39199, 59299,   143,\n",
            "         88764, 10167,   107,   107, 86229, 12574,   106,   100, 46646, 39199,\n",
            "             0,     0,     0,     0,     0,     0],\n",
            "        [  100, 11162,   118, 10387, 59571, 10107, 17826, 10171,   143, 36186,\n",
            "           131, 10139, 15785, 35403, 14086,   117, 12317, 10127, 12125,   143,\n",
            "         18067, 10131, 10986, 36186, 57846,   119, 40978, 10387, 12134,   144,\n",
            "           119,   119,   100, 40978, 10387, 12134],\n",
            "        [  100, 25044, 10114, 22611, 10103, 67176, 47297, 10108, 16674, 10572,\n",
            "         10142,   117, 13446, 12447, 10110, 18914, 24756,   117, 15060, 36283,\n",
            "         10140, 28189, 10139,   119,   119,   119,   119,   100, 16674, 10572,\n",
            "         10142,     0,     0,     0,     0,     0]]), tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0],\n",
            "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0],\n",
            "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1],\n",
            "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0]]), tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0],\n",
            "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0],\n",
            "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
            "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0]]), tensor([0, 0, 1, 2]))\n"
          ]
        }
      ],
      "source": [
        "for data in train_loader:\n",
        "  print(data)\n",
        "  break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "id": "um3C4Q3J3s9X"
      },
      "outputs": [],
      "source": [
        "# %time train_model_ABSA(train_loader, epoch)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "id": "Y5vISdVN3s9X"
      },
      "outputs": [],
      "source": [
        "model_ABSA = load_model(model_ABSA, f'{file_save_path}/{ABSA_file}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fE0ICxRh3s9Y",
        "outputId": "715f14f9-4182-4d7c-b3de-f2df229e28f4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CPU times: user 5.5 s, sys: 48.6 ms, total: 5.55 s\n",
            "Wall time: 5.51 s\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.64      0.74      0.69       497\n",
            "           1       0.73      0.59      0.65       710\n",
            "           2       0.83      0.87      0.85      1239\n",
            "\n",
            "    accuracy                           0.76      2446\n",
            "   macro avg       0.73      0.73      0.73      2446\n",
            "weighted avg       0.76      0.76      0.76      2446\n",
            "\n"
          ]
        }
      ],
      "source": [
        "%time x, y = test_model_ABSA(test_loader)\n",
        "print(classification_report(x, y, target_names=[str(i) for i in range(3)]))\n",
        "with open(f\"{file_save_path}/ABSA_epoch{epoch}_report.txt\", \"w\") as file:\n",
        "  file.write(classification_report(x, y, target_names=[str(i) for i in range(3)]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9fKXB7q0NTM9"
      },
      "source": [
        "# Morphs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zHjGgrfl3s9Y"
      },
      "source": [
        "# ATE + ABSA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "id": "IAzyFIlkUe5o"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "tfidf = TfidfVectorizer(min_df=0, max_features=512)\n",
        "tfidf_vectorizer = tfidf.fit([\" \".join(i[0]) for i in train_ds])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "id": "FwnXHl-V3s9Y"
      },
      "outputs": [],
      "source": [
        "def predict_model_ABSA(sentence, aspect, tokenizer):\n",
        "    # t1 = tokenizer.tokenize(sentence)\n",
        "    # t2 = tokenizer.tokenize(aspect)\n",
        "    t1 = tokenizer.morphs(sentence)\n",
        "    t2 = tokenizer.morphs(aspect)\n",
        "\n",
        "    word_pieces = ['[CLS]']\n",
        "    word_pieces += t1\n",
        "    word_pieces += ['[SEP]']\n",
        "    word_pieces += t2\n",
        "\n",
        "    segment_tensor = [0] + [0]*len(t1) + [0] + [1]*len(t2)\n",
        "\n",
        "    ids = tokenizer.convert_tokens_to_ids(word_pieces)\n",
        "\n",
        "    input_tensor = torch.tensor([ids]).to(DEVICE)\n",
        "    segment_tensor = torch.tensor(segment_tensor).to(DEVICE)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model_ABSA(input_tensor, None, None, segments_tensors=segment_tensor)\n",
        "        _, predictions = torch.max(outputs, dim=1)\n",
        "\n",
        "    return word_pieces, predictions, outputs\n",
        "\n",
        "def predict_model_ATE(sentence, tokenizer):\n",
        "    word_pieces = []\n",
        "    # tokens = tokenizer.tokenize(sentence)\n",
        "    tokens = tokenizer.morphs(sentence)\n",
        "    word_pieces += tokens\n",
        "\n",
        "    # ids = tokenizer.convert_tokens_to_ids(word_pieces)\n",
        "    ids = tfidf_vectorizer.transform([sentence]).toarray()\n",
        "    #@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
        "    #@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
        "    #@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
        "    #@@@@@@@@@@@@@@@@@@@@@@@@@여기 안됨@@@@@@@@@@@@@@@@@@@@@@@@@\n",
        "    #@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
        "    #@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
        "    #@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
        "    # ids array size를 512fh 변경할 것\n",
        "    input_tensor = torch.LongTensor(ids).to(DEVICE)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model_ATE(input_tensor, None, None)\n",
        "        _, predictions = torch.max(outputs, dim=2)\n",
        "    predictions = predictions[0].tolist()\n",
        "\n",
        "    return word_pieces, predictions, outputs\n",
        "\n",
        "def ATE_ABSA(text):\n",
        "    terms = []\n",
        "    word = \"\"\n",
        "    x, y, z = predict_model_ATE(text, mecab)\n",
        "    for i in range(len(y)):\n",
        "        if y[i] == 1:\n",
        "            if len(word) != 0:\n",
        "                terms.append(word.replace(\"##\",\"\").replace(\" \", \"\"))\n",
        "            word = x[i]\n",
        "        if y[i] == 2:\n",
        "            word += (\" \" + x[i])\n",
        "\n",
        "\n",
        "    if len(word) != 0:\n",
        "            terms.append(word.replace(\"##\",\"\"))\n",
        "\n",
        "    print(\"tokens:\", x)\n",
        "    print(\"ATE:\", terms)\n",
        "\n",
        "    # if len(terms) != 0:\n",
        "        # for i in terms:\n",
        "            # _, c, p = predict_model_ABSA(text, i, tokenizer)\n",
        "            # print(\"term:\", [i], \"class:\", [int(c)], \"ABSA:\", [float(p[0][0]), float(p[0][1]), float(p[0][2])])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "id": "gg9Fm2WN3s9Y"
      },
      "outputs": [],
      "source": [
        "model_ABSA = load_model(model_ABSA, f'{file_save_path}/{ABSA_file}')\n",
        "model_ATE = load_model(model_ATE, f'{file_save_path}/{ATE_file}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "id": "3Wh7k2sMAk6l"
      },
      "outputs": [],
      "source": [
        "import re"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bv63mSAH3s9Y",
        "outputId": "58c8e9b8-5198-4069-c2cf-59404cf4f755"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tokens: ['깔끔', '하', '게', '부직포', '포장', '으로', '되', '어', '있', '어서', '그냥', '뜨거운', '물', '에', '풍덩', '넣', '어', '놓', '고', '좀', '휘젓', '어', '주', '면', '금방', '우러난다', '.']\n",
            "ATE: []\n"
          ]
        }
      ],
      "source": [
        "text = \"깔끔하게 부직포 포장으로 되어 있어서 그냥 뜨거운 물에 풍덩 넣어놓고 좀 휘젓어주면 금방 우러난다.\"\n",
        "ATE_ABSA(text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ThVN6-oTu-BB",
        "outputId": "1a3a930c-f7b2-44e2-cb5e-d1191d3cfe87"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tokens: ['제품', '만족', '도', '94', '%', '제품', '이', '라', '믿', '고', '사용', '해', '본', '#', '산다화', '클렌징', '오일']\n",
            "ATE: []\n"
          ]
        }
      ],
      "source": [
        "ATE_ABSA(\"제품만족도 94% 제품이라 믿고 사용해본 #산다화클렌징오일\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C2_E0V-X3s9Y",
        "outputId": "8704c2bf-2a9b-4381-c333-daeaf132616d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tokens: ['목욕', '할', '때', '마다', '넣', '어', '봤', '는데', '(', '샤워', '는', '자주', '해도', '목욕', '은', '그렇게', '자주', '가', '아님', '.', '.', '이것', '도', '약재', '는', '약재', '이', '므로', '용법', '은', '알', '아서', ';;)', '신선', '한', '한', '약풀', '냄새', '가', '욕실', '에', '퍼져서', '기분', '이', '좋', '아', '졌', '다', '.']\n",
            "ATE: []\n"
          ]
        }
      ],
      "source": [
        "text = \"목욕할 때마다 넣어봤는데(샤워는 자주 해도 목욕은 그렇게 자주가 아님.. 이것도 약재는 약재이므로 용법은 알아서;;)신선한 한약풀 냄새가 욕실에 퍼져서 기분이 좋아졌다.\"\n",
        "\n",
        "ATE_ABSA(text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "alcQ1-BuAhOl",
        "outputId": "b53dfb9b-21f1-4a43-afb1-44fbc981fab8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tokens: ['고밀도', '폼', '소재', '라', '.', '.', '빈틈', '이', '없', '고', '커버', '만', '관리', '하', '면', '되', '고', '항균', '폼', '이', '내재', '되', '어', '.', '항균', '이', '가능', '해요']\n",
            "ATE: []\n"
          ]
        }
      ],
      "source": [
        "text = \"고밀도 폼 소재라..빈틈이 없고 커버만 관리하면 되고, 항균폼이 내재되어 99.9% 항균이 가능해요~\"\n",
        "text = re.sub('[^가-힣a-z.]', ' ', text)\n",
        "ATE_ABSA(text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g-95YawnItCD",
        "outputId": "35c828b3-2562-44e5-8582-bcc715b2f9fe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tokens: ['나', '는', '사람', '이', '좋', '아']\n",
            "ATE: []\n"
          ]
        }
      ],
      "source": [
        "text = \"\"\"나는 사람이 좋아\"\"\"\n",
        "ATE_ABSA(text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Muj_Yntb3s9Z"
      },
      "source": [
        "# Cyberpunk 2077 - Xbox One\n",
        "\n",
        "https://www.amazon.com/-/zh_TW/Cyberpunk-2077-Xbox-One/product-reviews/B07DJW4WZC/ref=cm_cr_arp_d_paging_btm_next_2?ie=UTF8&reviewerType=all_reviews&pageNumber=2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GUTpWCKQ3s9Z",
        "outputId": "5a5fe144-aead-4727-f1b5-07dbe35685a1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tokens: ['Spent', '5', 'hours', 'downloading', 'updates', '.']\n",
            "ATE: []\n"
          ]
        }
      ],
      "source": [
        "text = \"Spent 5 hours downloading updates.\"\n",
        "ATE_ABSA(text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k1BKlrVK3s9Z",
        "outputId": "0c625ea7-54ba-4ddd-be3b-d6d01a050e25"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tokens: ['Install', 'is', 'buggy', ',', 'so', 'after', 'downloading', 'a', 'day', 'one', 'patch', 'that', \"'\", 's', 'nearly', '3', 'times', 'the', 'size', 'of', 'the', 'game', ',', 'it', 'glitched', 'on', 'the', 'CDs', 'and', 'had', 'to', 'reinstall', 'the', 'game', 'from', 'scratch', '.']\n",
            "ATE: []\n"
          ]
        }
      ],
      "source": [
        "text = \"Install is buggy, so after downloading a day one patch that's nearly 3 times the size of the game, it glitched on the CDs and had to reinstall the game from scratch.\"\n",
        "ATE_ABSA(text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7bUw7Rti3s9Z",
        "outputId": "20999c2c-a09a-4946-871c-57d138d78498"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tokens: ['Cyberpunk', '2077', 'freezes', 'constantly', ',', 'frame', 'rates', 'are', 'terrible', ',', 'and', 'it', \"'\", 's', 'extremely', 'frustrating', 'to', 'try', 'to', 'play', '.']\n",
            "ATE: []\n"
          ]
        }
      ],
      "source": [
        "text = \"Cyberpunk 2077 freezes constantly, frame rates are terrible, and it's extremely frustrating to try to play.\"\n",
        "ATE_ABSA(text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ckmkALR-3s9e",
        "outputId": "d25d4278-85cf-41da-a3c2-ced7bf76253b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tokens: ['Cyberpunk', '2077', 'is', 'completely', 'unplayable', 'on', 'xbox', 'one', '.', 'They', 'should', 'have', 'never', 'released', 'this', 'for', 'current', 'gen', '.']\n",
            "ATE: []\n"
          ]
        }
      ],
      "source": [
        "text = \"Cyberpunk 2077 is completely unplayable on xbox one. They should have never released this for current gen.\"\n",
        "ATE_ABSA(text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bVjwSBEE3s9f",
        "outputId": "1dfa3cc1-92fa-4495-d6ea-f6966536e832"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tokens: ['It', '’', 's', 'just', 'a', 'cash', 'grab', ',', 'the', 'game', 'crashes', 'constantly', ',', 'runs', 'at', 'like', '20', 'fps', ',', 'half', 'the', 'environment', 'and', 'characters', 'only', 'load', 'when', 'you', '’', 're', 'three', 'feet', 'away', 'from', 'them', '.', 'Unless', 'you', '’', 're', 'in', 'a', 'small', 'space', 'the', 'game', 'looks', 'awful', '.', 'The', 'worst', 'game', 'i', '’', 've', 'ever', 'played', 'in', 'years', 'visually', '.', 'It', 'looks', 'worse', 'than', 'later', 'xbox', '360', 'games', '.']\n",
            "ATE: []\n"
          ]
        }
      ],
      "source": [
        "text = \"It’s just a cash grab, the game crashes constantly, runs at like 20 fps, half the environment and characters only load when you’re three feet away from them. Unless you’re in a small space the game looks awful. The worst game i’ve ever played in years visually. It looks worse than later xbox 360 games.\"\n",
        "ATE_ABSA(text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i1GMM0ND3s9g",
        "outputId": "ddab512e-791e-4219-f5cf-f76d55d4a511"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tokens: ['CD', 'Projekt', 'Red', 'should', 'have', 'just', 'abandoned', 'the', 'current', 'gen', 'consoles', 'instead', 'of', 'cheating', 'people', 'out', 'of', 'their', 'money', '.']\n",
            "ATE: []\n"
          ]
        }
      ],
      "source": [
        "text = \"CD Projekt Red should have just abandoned the current gen consoles instead of cheating people out of their money.\"\n",
        "ATE_ABSA(text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {
        "id": "gd-1hZxx3s9g"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
